---
title: "Allegro Targeted Genotyping variant calling"
output:
  github_document: default
---

```{r, echo = F}
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

# From https://github.com/rstudio/rmarkdown/issues/294
knitr::opts_chunk$set(error = T)
```

# Overview
This workflow calls variants using sequence data obtained from fecal swabs using the Allegro Targeted Genotyping (ATG) assay. This work is published at https://doi.org/10.1007/s12686-022-01259-2. Overall, we will:
* 1. Call variants using the suggested pipeline by Nugen
  + a. Run trim_galore to remove the first 40 nucleotides
  + b. Perform single end read alignment to the EquCab2 reference genome.
  + c. Call variants using the mpileup algorithm using bcftools.
  + d. Perform filtering to remove null alleles and low quality genotypes. 
2. Use the VariantsToTable utility from GATK to get our final outputs (genotype and depth tables) for further summary statistics.


### Set up
First, download miniconda3 (documentation available at https://docs.conda.io/en/latest/miniconda.html). Then create and activate the environment using the commands below. 
```{zsh eval = F}
conda env create --file environment.yml
conda activate variant_analysis
```

Next we will need to download the EquCab2 reference genome. I am sure an online version could be accessed, but I found it handy to have it available locally. The file is large (about 7.7g compressed), so beware. The file can be downloaded using:

```{zsh eval = F}
wget http://igenomes.illumina.com.s3-website-us-east-1.amazonaws.com/Equus_caballus/Ensembl/EquCab2/Equus_caballus_Ensembl_EquCab2.tar.gz
tar -xzvf Equus_caballus_Ensembl_EquCab2.tar.gz
```

And lastly, before getting started with the workflow, I will create a VCF file of the reference genotype files (which are currently in PLINK format), along with changing chromosome 32 to X and sorting the VCF file using a script I found on github.

```{zsh engine.opts='-l'}
plink --file data/SNPchip_genotypes_in_Plink_format/Sable_Nugen_Illumina_Affy_combined \
    --horse --recode vcf # makes plink.vcf
# Change chromosome 32 to X, then sort VCF by chromosome & position
sed 's/^32/X/g' plink.vcf | ./extra_scripts/vcfsort.sh - > ref.vcf
rm plink.* # remove unnecessary files from plink
```

I also want to create a file that keeps track of read counts throughout the pipeline, starting with raw read counts. I remove the sample statistics file in case it already exists, since I will append counts to it during each step.
```{bash engine.opts='-l'}
rm sampleStatistics.csv
for sample in seqs/*fastq.gz
do
    base=`basename -s "_L001_R1_001.fastq.gz" $sample`
    zcat $sample | echo $base $((`wc -l`/4)) >> sampleStatistics.csv
done
```

### TrimGalore
We must remove the first 40 base pairs from our sequences as this contains sequences that are not of biological relevance. For this we use trim_galore, and we relaxed some of the parameters to try and increase the number of reads retained, but this does not have a large effect. Below are the first 25 lines of output which summarize the effects of running trim_galore on the first input file:
```{zsh engine.opts='-l', output.lines=25}
for sample in seqs/*R1_001.fastq.gz; do
    echo $sample
    base=`basename -s "_L001_R1_001.fastq.gz" $sample`
    echo $base
    trim_galore --length 15 --stringency 7 --clip_R1 40 \
        $sample -o intermediates
done
```

Then append these read counts to our sample statistics file by first writing them to a temporary text file.
```{bash engine.opts='-l'}
rm tmp.txt
for sample in intermediates/*_trimmed.fq.gz; do
    base=`basename -s "_L001_R1_001_trimmed.fastq.gz" $sample`
    #zcat $sample | echo $base $((`wc -l`/4)) | cut -f2 -d' '
    zcat $sample | echo $base $((`wc -l`/4)) | cut -f2 -d' ' >> tmp.trim.txt
done

# Append to sample statistics file, create a temporary file for renaming
paste -d " " sampleStatistics.csv tmp.trim.txt > tmp.txt
mv tmp.txt sampleStatistics.csv
rm tmp.trim.txt
```

### Single end read alignment
Next we will align the forward reads to the EquCab2 reference genome. We have enabled very sensitive local alignment as it appears to retain a similar amount of reads compared to the less sensitive parameters. Then we move the .bam outputs to the intermediate file folder generated by trim_galore. I start by specifying where the reference genome is located. Specifically, I refer to the bowtie2 reference genome.
```{zsh engine.opts='-l', output.lines=25}
BOWTIE2_REFERENCE="/home/stefan/Documents/Research/msc/EquCab2/Sequence/Bowtie2Index/genome"

for sample in intermediates/*trimmed.fq.gz ; do
    echo $sample
    base=`basename -s "_trimmed.fq.gz" $sample`
    echo $base
    bowtie2 -p 2 --very-sensitive-local -U $sample \
        -x $BOWTIE2_REFERENCE | samtools sort -o $base".bam"
done

mv *.bam intermediates
```

Now we get the read counts again.
```{bash engine.opts='-l'}
rm tmp.map.txt
# Number of reads that mapped to reference genome (EquCab2)
for sample in intermediates/*.bam; do
    base=`basename -s ".bam" $sample`
    echo "`samtools view -c -F 260 $sample` " >> tmp.map.txt
done

paste -d " " sampleStatistics.csv tmp.map.txt > tmp.txt
mv tmp.txt sampleStatistics.csv
rm tmp.map.txt
```

Something that has caused a lot of headaches for me in the past has been the necessity of adding read group information to bam files. Without this step, information like the sample name is lost in the .bam file which creates a lot of problems downstream, so we can use the GATK tool "AddOrReplaceReadGroups" here. Again, we move the outputs to the intermediate file folder.
```{zsh engine.opts='-l', output.lines=25}
for sample in intermediates/*.bam ; do
    echo $sample
    base=`basename -s ".bam" $sample`
    echo $base

    gatk AddOrReplaceReadGroups \
        I=$sample \
        O=$base"_rg.bam" \
        RGID=1 \
        RGLB=lib1 \
        RGPL=nugen \
        RGPU=unit1 \
        RGSM=$sample

    # Index alignment files
    samtools index $base"_rg.bam"
done

mv *_rg.bam* intermediates
```

### Call variants using mpileup
Now we can call variants, but since we have a set of variants that act as our 'reference data set', we can create a list to input to variant calling so that mpileup only outputs these particular sites. To do that, we have to compress and index the vcf file we generated at the start of the workflow. Then we create the targets file, and I like to have the file decompressed so I unzip it again.

```{zsh engine.opts='-l'}
bgzip ref.vcf
tabix -p vcf ref.vcf.gz

# Create targets file from reference variants
bcftools query -f '%CHROM\t%POS\t%REF,%ALT\n' ref.vcf.gz | \
    bgzip -c > targets.tsv.gz && tabix -s1 -b2 targets.tsv.gz

gunzip ref.vcf.gz
```

Now we can call the variants:
```{zsh engine.opts='-l'}
FASTA_REFERENCE="/home/stefan/Documents/Research/msc/EquCab2/Sequence/WholeGenomeFasta/genome.fa"
samtools merge merged.bam intermediates/*_rg.bam
bcftools mpileup -d1000000 --skip-indels -Ov merged.bam \
    --fasta-ref $FASTA_REFERENCE -a FORMAT/DP \
    --targets-file targets.tsv.gz -o merged.mpileup 
```

For some quality control, we can remove null alleles by using the -v parameter, and then 
```{zsh engine.opts='-l'}
bcftools call merged.mpileup -v -f GQ -t targets.tsv.gz -O v -m > merged.vcf

bcftools filter -S . -e "FMT/GQ<20" merged.vcf -o merged.filt.vcf
```

### Create genotype and depth count tables
Now we can use the utility from GATK to create nicely formatted tables detailing the genotypes and read depth for each genotype. First I index the vcf files again (for use with GATK), sort all output VCFs and then do some quick text formatting with sed. 
```{zsh engine.opts='-l'}
bgzip merged.mpileup ; tabix -p vcf merged.mpileup.gz ; gunzip merged.mpileup
bgzip merged.vcf ; tabix -p vcf merged.vcf.gz ; gunzip merged.vcf
bgzip merged.filt.vcf ; tabix -p vcf merged.filt.vcf.gz ; gunzip merged.filt.vcf

#./vcfsort.sh nugen.vcf > nugen.sort.vcf
./extra_scripts/vcfsort.sh merged.mpileup > merged.sort.mpileup
./extra_scripts/vcfsort.sh merged.vcf > merged.sort.vcf
./extra_scripts/vcfsort.sh merged.filt.vcf > merged.filt.sort.vcf
rm merged.mpileup ; rm merged.vcf ; rm merged.filt.vcf

# Just some text formatting
sed -i 's;intermediates/;;g' merged.sort.mpileup
sed -i 's/_L001_R1_001.bam//g' merged.sort.mpileup
sed -i 's;intermediates/;;g' merged.sort.vcf
sed -i 's/_L001_R1_001.bam//g' merged.sort.vcf
sed -i 's;intermediates/;;g' merged.filt.sort.vcf
sed -i 's/_L001_R1_001.bam//g' merged.filt.sort.vcf
```

And then I get all of the desired output tables.
```{zsh engine.opts='-l', output.lines=25}
gatk VariantsToTable -V merged.sort.mpileup -F CHROM -F POS \
    -F REF -F ALT -GF DP -O merged.depths.mpileup.table

gatk VariantsToTable -V merged.sort.vcf -F CHROM -F POS \
    -F REF -F ALT -GF DP -O merged.depths.table

# For the final table, get the genotypes as well 
gatk VariantsToTable -V merged.filt.sort.vcf -F CHROM -F POS \
    -F REF -F ALT -GF DP -O merged.filt.depths.table
gatk VariantsToTable -V merged.filt.sort.vcf -F CHROM -F POS \
    -F REF -F ALT -GF GT -O merged.filt.genotypes.table
    
# do the same for reference vcf genotypes
gatk VariantsToTable -V ref.vcf -F CHROM -F POS -F REF -F ALT -GF GT \
    -O ref.genotypes.table

gatk VariantsToTable -V ref.vcf -F CHROM -F POS -F REF -F ALT -GF DP \
    -O ref.depths.table
```

### Final formatting and clean-up
Here I run an Rscript I made to clean-up some of the formatting (things like column names and so on) in the final output files.
```{zsh engine.opts='-l', message=FALSE, warning=FALSE}
cd extra_scripts ; Rscript tableCleaningAndFormatting.R ; cd ..
```

Here is some optional clean up to move or remove files we no longer need.
```{zsh engine.opts='-l'}
mv *table data
mv sampleStatistics.csv data

rm -rf intermediates
rm targets*
rm ref.vcf*
rm merged.*
```

And lastly, deactivate the conda environment. This workflow will have created the genotype + depth tables (now in the "data" folder) necessary to run the post-processing analysis for this manuscript. 
```{zsh, eval = F}
conda deactivate
```